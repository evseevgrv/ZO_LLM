2025-04-12 15:26:53,464 - INFO - Sample train set 1500/67349
2025-04-12 15:26:53,465 - INFO - ... including dev set 500 samples
2025-04-12 15:26:53,465 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
34
2025-04-12 15:27:00,280 - INFO - Done with 6.82s
2025-04-12 15:27:00,566 - INFO - Inject lora to: model.decoder.layers.0.self_attn
2025-04-12 15:27:00,739 - INFO - Inject lora to: model.decoder.layers.1.self_attn
2025-04-12 15:27:00,886 - INFO - Inject lora to: model.decoder.layers.2.self_attn
2025-04-12 15:27:00,982 - INFO - Inject lora to: model.decoder.layers.3.self_attn
2025-04-12 15:27:01,088 - INFO - Inject lora to: model.decoder.layers.4.self_attn
2025-04-12 15:27:01,191 - INFO - Inject lora to: model.decoder.layers.5.self_attn
2025-04-12 15:27:01,292 - INFO - Inject lora to: model.decoder.layers.6.self_attn
2025-04-12 15:27:01,389 - INFO - Inject lora to: model.decoder.layers.7.self_attn
2025-04-12 15:27:01,487 - INFO - Inject lora to: model.decoder.layers.8.self_attn
2025-04-12 15:27:01,590 - INFO - Inject lora to: model.decoder.layers.9.self_attn
2025-04-12 15:27:01,690 - INFO - Inject lora to: model.decoder.layers.10.self_attn
2025-04-12 15:27:01,790 - INFO - Inject lora to: model.decoder.layers.11.self_attn
2025-04-12 15:27:01,887 - INFO - Inject lora to: model.decoder.layers.12.self_attn
2025-04-12 15:27:02,001 - INFO - Inject lora to: model.decoder.layers.13.self_attn
2025-04-12 15:27:02,100 - INFO - Inject lora to: model.decoder.layers.14.self_attn
2025-04-12 15:27:02,197 - INFO - Inject lora to: model.decoder.layers.15.self_attn
2025-04-12 15:27:02,294 - INFO - Inject lora to: model.decoder.layers.16.self_attn
2025-04-12 15:27:02,390 - INFO - Inject lora to: model.decoder.layers.17.self_attn
2025-04-12 15:27:02,486 - INFO - Inject lora to: model.decoder.layers.18.self_attn
2025-04-12 15:27:02,597 - INFO - Inject lora to: model.decoder.layers.19.self_attn
2025-04-12 15:27:02,697 - INFO - Inject lora to: model.decoder.layers.20.self_attn
2025-04-12 15:27:02,796 - INFO - Inject lora to: model.decoder.layers.21.self_attn
2025-04-12 15:27:02,914 - INFO - Inject lora to: model.decoder.layers.22.self_attn
2025-04-12 15:27:03,014 - INFO - Inject lora to: model.decoder.layers.23.self_attn
2025-04-12 15:27:03,116 - INFO - Dev samples: 500
2025-04-12 15:27:03,117 - INFO - Train samples: 1000
2025-04-12 15:27:03,117 - INFO - Eval sample length is 872
2025-04-12 15:27:03,118 - INFO - Tokenizing training samples...
2025-04-12 15:27:04,037 - INFO - Done with 0.92s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-04-12 15:27:04,050 - INFO - ***** Running training *****
2025-04-12 15:27:04,050 - INFO -   Num examples = 1000
2025-04-12 15:27:04,050 - INFO -   Num Epochs = 318
2025-04-12 15:27:04,050 - INFO -   Instantaneous batch size per device = 16
2025-04-12 15:27:04,050 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-04-12 15:27:04,051 - INFO -   Gradient Accumulation steps = 1
2025-04-12 15:27:04,051 - INFO -   Total optimization steps = 20000
2025-04-12 15:27:04,051 - INFO -   Number of trainable parameters = 1572864
  0%|                                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 736, in <module>
    main()
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 688, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 573, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 544, in _inner_training_loop
    tr_loss_step = self.zo_muon_step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 1159, in zo_muon_step
    optimizer.step()
NameError: name 'optimizer' is not defined
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------