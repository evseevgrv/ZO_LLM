2025-04-16 17:26:18,755 - INFO - Sample train set 1500/67349
2025-04-16 17:26:18,756 - INFO - ... including dev set 500 samples
2025-04-16 17:26:18,756 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
78
2025-04-16 17:26:23,727 - INFO - Done with 4.97s
2025-04-16 17:26:24,020 - INFO - Dev samples: 500
2025-04-16 17:26:24,020 - INFO - Train samples: 1000
2025-04-16 17:26:24,020 - INFO - Eval sample length is 872
2025-04-16 17:26:24,021 - INFO - Tokenizing training samples...
2025-04-16 17:26:24,896 - INFO - Done with 0.88s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-04-16 17:26:24,904 - INFO - ***** Running training *****
2025-04-16 17:26:24,905 - INFO -   Num examples = 1000
2025-04-16 17:26:24,905 - INFO -   Num Epochs = 318
2025-04-16 17:26:24,905 - INFO -   Instantaneous batch size per device = 16
2025-04-16 17:26:24,905 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-04-16 17:26:24,905 - INFO -   Gradient Accumulation steps = 1
2025-04-16 17:26:24,905 - INFO -   Total optimization steps = 20000
2025-04-16 17:26:24,906 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                         | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
  0%|                                                                                                                                              | 1/20000 [00:08<47:07:44,  8.48s/it]

  0%|                                                                                                                                              | 2/20000 [00:17<50:05:31,  9.02s/it]

  0%|                                                                                                                                              | 3/20000 [00:27<51:04:18,  9.19s/it]

  0%|                                                                                                                                              | 4/20000 [00:36<51:34:49,  9.29s/it]

  0%|                                                                                                                                              | 5/20000 [00:46<51:50:20,  9.33s/it]
{'peak_mem': 5.889943599700928, 'step_consumption': 9413.347005844116, 'epoch': 0.08}


  0%|                                                                                                                                              | 7/20000 [01:04<51:34:00,  9.29s/it]
{'peak_mem': 5.889943599700928, 'step_consumption': 9084.327697753906, 'epoch': 0.11}


  0%|                                                                                                                                              | 9/20000 [01:22<50:40:33,  9.13s/it]
{'peak_mem': 5.889943599700928, 'step_consumption': 8974.774360656738, 'epoch': 0.14}
{'loss': 348.0897, 'learning_rate': 0.001, 'epoch': 0.16}


  0%|                                                                                                                                             | 11/20000 [01:40<50:25:11,  9.08s/it]
{'peak_mem': 5.890035152435303, 'step_consumption': 9031.744241714478, 'epoch': 0.17}


  0%|                                                                                                                                             | 13/20000 [01:58<50:18:21,  9.06s/it]
{'peak_mem': 5.890042781829834, 'step_consumption': 9034.737586975098, 'epoch': 0.21}


  0%|                                                                                                                                             | 15/20000 [02:16<50:16:21,  9.06s/it]
{'peak_mem': 5.890065670013428, 'step_consumption': 9040.24338722229, 'epoch': 0.24}


  0%|                                                                                                                                             | 17/20000 [02:35<50:13:49,  9.05s/it]

  0%|▏                                                                                                                                            | 18/20000 [02:44<50:18:31,  9.06s/it]

  0%|▏                                                                                                                                            | 19/20000 [02:53<50:21:13,  9.07s/it]

  0%|▏                                                                                                                                            | 20/20000 [03:02<50:22:20,  9.08s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.32}

  0%|▏                                                                                                                                            | 21/20000 [03:11<50:22:43,  9.08s/it]

  0%|▏                                                                                                                                            | 22/20000 [03:20<50:22:22,  9.08s/it]

  0%|▏                                                                                                                                            | 23/20000 [03:29<50:14:55,  9.06s/it]

  0%|▏                                                                                                                                            | 24/20000 [03:38<50:11:51,  9.05s/it]

  0%|▏                                                                                                                                            | 25/20000 [03:47<50:12:38,  9.05s/it]

  0%|▏                                                                                                                                            | 26/20000 [03:56<50:10:02,  9.04s/it]

  0%|▏                                                                                                                                            | 27/20000 [04:05<50:23:42,  9.08s/it]

  0%|▏                                                                                                                                            | 28/20000 [04:14<50:24:37,  9.09s/it]
{'peak_mem': 6.011800289154053, 'step_consumption': 9091.60566329956, 'epoch': 0.44}


  0%|▏                                                                                                                                            | 30/20000 [04:32<50:16:54,  9.06s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.48}
{'peak_mem': 6.011800289154053, 'step_consumption': 9017.205238342285, 'epoch': 0.48}


  0%|▏                                                                                                                                            | 32/20000 [04:51<50:16:17,  9.06s/it]
{'peak_mem': 6.011800289154053, 'step_consumption': 9050.684213638306, 'epoch': 0.51}


  0%|▏                                                                                                                                            | 34/20000 [05:09<50:19:10,  9.07s/it]

  0%|▏                                                                                                                                            | 35/20000 [05:18<50:26:08,  9.09s/it]

  0%|▎                                                                                                                                            | 36/20000 [05:27<50:23:00,  9.09s/it]

  0%|▎                                                                                                                                            | 37/20000 [05:36<50:19:07,  9.07s/it]
  0%|▎                                                                                                                                            | 37/20000 [05:36<50:19:07,  9.07s/it]Traceback (most recent call last):
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 736, in <module>
    main()
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 688, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 573, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 544, in _inner_training_loop
    tr_loss_step = self.zo_muon_step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 1134, in zo_muon_step
    g_ortho = zeropower_via_newtonschulz5(g, steps=10)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 147, in zeropower_via_newtonschulz5
    X = a * X + B @ X
KeyboardInterrupt