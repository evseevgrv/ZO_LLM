
Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35.3k/35.3k [00:00<00:00, 20.2MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.11M/3.11M [00:00<00:00, 4.84MB/s]
Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72.8k/72.8k [00:00<00:00, 179kB/s]
Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 148k/148k [00:00<00:00, 374kB/s]
Generating train split: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67349/67349 [00:00<00:00, 808563.53 examples/s]
Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 391630.06 examples/s]
Generating test split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1821/1821 [00:00<00:00, 651969.92 examples/s]
2025-02-25 00:47:15,196 - INFO - Sample train set 1200/67349
2025-02-25 00:47:15,197 - INFO - ... including dev set 200 samples
2025-02-25 00:47:15,197 - INFO - Loading model with FP32...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 651/651 [00:00<00:00, 924kB/s]
9

pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 251M/251M [00:02<00:00, 114MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:00<00:00, 420kB/s]
2025-02-25 00:47:22,406 - INFO - Done with 7.21s
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [00:00<00:00, 2.70MB/s]
vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 2.60MB/s]
merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 4.00MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1.65MB/s]
2025-02-25 00:47:24,607 - INFO - Dev samples: 200
2025-02-25 00:47:24,607 - INFO - Train samples: 1000
2025-02-25 00:47:24,607 - INFO - Eval sample length is 500
2025-02-25 00:47:24,609 - INFO - Tokenizing training samples...
2025-02-25 00:47:25,262 - INFO - Done with 0.65s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Traceback (most recent call last):
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 729, in <module>
    main()
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 681, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 566, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 324, in _inner_training_loop
    assert args.lr_scheduler_type == 'constant', "we did not implement lr_schedule."
AssertionError: we did not implement lr_schedule.