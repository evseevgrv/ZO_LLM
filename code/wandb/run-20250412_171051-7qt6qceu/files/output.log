2025-04-12 17:11:02,219 - INFO - Sample train set 1500/67349
2025-04-12 17:11:02,221 - INFO - ... including dev set 500 samples
2025-04-12 17:11:02,222 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
23
2025-04-12 17:11:11,606 - INFO - Done with 9.38s
2025-04-12 17:11:12,009 - INFO - Inject lora to: model.decoder.layers.0.self_attn
2025-04-12 17:11:12,180 - INFO - Inject lora to: model.decoder.layers.1.self_attn
2025-04-12 17:11:12,291 - INFO - Inject lora to: model.decoder.layers.2.self_attn
2025-04-12 17:11:12,417 - INFO - Inject lora to: model.decoder.layers.3.self_attn
2025-04-12 17:11:12,534 - INFO - Inject lora to: model.decoder.layers.4.self_attn
2025-04-12 17:11:12,632 - INFO - Inject lora to: model.decoder.layers.5.self_attn
2025-04-12 17:11:12,863 - INFO - Inject lora to: model.decoder.layers.6.self_attn
2025-04-12 17:11:12,992 - INFO - Inject lora to: model.decoder.layers.7.self_attn
2025-04-12 17:11:13,117 - INFO - Inject lora to: model.decoder.layers.8.self_attn
2025-04-12 17:11:13,255 - INFO - Inject lora to: model.decoder.layers.9.self_attn
2025-04-12 17:11:13,430 - INFO - Inject lora to: model.decoder.layers.10.self_attn
2025-04-12 17:11:13,538 - INFO - Inject lora to: model.decoder.layers.11.self_attn
2025-04-12 17:11:13,644 - INFO - Inject lora to: model.decoder.layers.12.self_attn
2025-04-12 17:11:13,817 - INFO - Inject lora to: model.decoder.layers.13.self_attn
2025-04-12 17:11:13,951 - INFO - Inject lora to: model.decoder.layers.14.self_attn
2025-04-12 17:11:14,061 - INFO - Inject lora to: model.decoder.layers.15.self_attn
2025-04-12 17:11:14,177 - INFO - Inject lora to: model.decoder.layers.16.self_attn
2025-04-12 17:11:14,321 - INFO - Inject lora to: model.decoder.layers.17.self_attn
2025-04-12 17:11:14,475 - INFO - Inject lora to: model.decoder.layers.18.self_attn
2025-04-12 17:11:14,613 - INFO - Inject lora to: model.decoder.layers.19.self_attn
2025-04-12 17:11:14,710 - INFO - Inject lora to: model.decoder.layers.20.self_attn
2025-04-12 17:11:14,811 - INFO - Inject lora to: model.decoder.layers.21.self_attn
2025-04-12 17:11:14,908 - INFO - Inject lora to: model.decoder.layers.22.self_attn
2025-04-12 17:11:15,004 - INFO - Inject lora to: model.decoder.layers.23.self_attn
2025-04-12 17:11:15,217 - INFO - Dev samples: 500
2025-04-12 17:11:15,218 - INFO - Train samples: 1000
2025-04-12 17:11:15,218 - INFO - Eval sample length is 872
2025-04-12 17:11:15,218 - INFO - Tokenizing training samples...
2025-04-12 17:11:16,438 - INFO - Done with 1.22s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-04-12 17:11:16,474 - INFO - ***** Running training *****
2025-04-12 17:11:16,479 - INFO -   Num examples = 1000
2025-04-12 17:11:16,479 - INFO -   Num Epochs = 318
2025-04-12 17:11:16,479 - INFO -   Instantaneous batch size per device = 16
2025-04-12 17:11:16,480 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-04-12 17:11:16,480 - INFO -   Gradient Accumulation steps = 1
2025-04-12 17:11:16,481 - INFO -   Total optimization steps = 20000
2025-04-12 17:11:16,482 - INFO -   Number of trainable parameters = 1572864
  0%|                                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                                                             | 1/20000 [00:01<9:35:12,  1.73s/it]
### layer-wise gradient sparsity = None

  0%|                                                                                                                                                             | 2/20000 [00:02<8:02:44,  1.45s/it]
{'peak_mem': 2.9677186012268066, 'step_consumption': 1718.5847759246826, 'epoch': 0.02}

  0%|                                                                                                                                                             | 4/20000 [00:04<5:27:55,  1.02it/s]
{'peak_mem': 3.3793768882751465, 'step_consumption': 763.174295425415, 'epoch': 0.05}
{'peak_mem': 3.3793768882751465, 'step_consumption': 727.7154922485352, 'epoch': 0.06}

  0%|                                                                                                                                                             | 7/20000 [00:06<4:35:56,  1.21it/s]
{'peak_mem': 3.3793768882751465, 'step_consumption': 834.6996307373047, 'epoch': 0.1}

  0%|                                                                                                                                                            | 10/20000 [00:08<3:42:04,  1.50it/s]
{'peak_mem': 3.3793768882751465, 'step_consumption': 868.5173988342285, 'epoch': 0.13}
{'peak_mem': 3.3793768882751465, 'step_consumption': 362.16020584106445, 'epoch': 0.14}
{'loss': 1.0814, 'learning_rate': 0.01, 'epoch': 0.16}
{'peak_mem': 3.3793768882751465, 'step_consumption': 649.4536399841309, 'epoch': 0.16}

  0%|                                                                                                                                                            | 15/20000 [00:11<2:31:07,  2.20it/s]
{'peak_mem': 3.3793768882751465, 'step_consumption': 390.8271789550781, 'epoch': 0.19}
{'peak_mem': 3.3793768882751465, 'step_consumption': 464.02645111083984, 'epoch': 0.21}
{'peak_mem': 3.3793768882751465, 'step_consumption': 358.4327697753906, 'epoch': 0.22}
{'peak_mem': 3.3793768882751465, 'step_consumption': 334.46288108825684, 'epoch': 0.24}

  0%|▏                                                                                                                                                           | 21/20000 [00:13<2:01:39,  2.74it/s]
{'peak_mem': 3.3793768882751465, 'step_consumption': 356.78625106811523, 'epoch': 0.27}
{'peak_mem': 3.531747817993164, 'step_consumption': 380.60760498046875, 'epoch': 0.29}
{'peak_mem': 3.531747817993164, 'step_consumption': 351.5286445617676, 'epoch': 0.3}
{'loss': 1.0582, 'learning_rate': 0.01, 'epoch': 0.32}
{'peak_mem': 3.531747817993164, 'step_consumption': 357.8610420227051, 'epoch': 0.32}
{'peak_mem': 3.531747817993164, 'step_consumption': 342.87214279174805, 'epoch': 0.33}

  0%|▏                                                                                                                                                           | 27/20000 [00:15<1:54:16,  2.91it/s]
{'peak_mem': 3.531747817993164, 'step_consumption': 282.17267990112305, 'epoch': 0.37}
{'peak_mem': 3.531747817993164, 'step_consumption': 300.77290534973145, 'epoch': 0.38}
{'peak_mem': 3.531747817993164, 'step_consumption': 336.05170249938965, 'epoch': 0.4}
{'peak_mem': 3.531747817993164, 'step_consumption': 301.48935317993164, 'epoch': 0.41}
{'peak_mem': 3.531747817993164, 'step_consumption': 385.5717182159424, 'epoch': 0.43}

  0%|▏                                                                                                                                                           | 31/20000 [00:16<2:22:13,  2.34it/s]
{'peak_mem': 3.531747817993164, 'step_consumption': 457.29899406433105, 'epoch': 0.46}
{'loss': 0.9059, 'learning_rate': 0.01, 'epoch': 0.48}
{'peak_mem': 3.531747817993164, 'step_consumption': 444.6249008178711, 'epoch': 0.48}
{'peak_mem': 3.531747817993164, 'step_consumption': 502.3620128631592, 'epoch': 0.49}

  0%|▎                                                                                                                                                           | 35/20000 [00:19<3:05:55,  1.79it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 596.0650444030762, 'epoch': 0.52}
{'peak_mem': 3.6841378211975098, 'step_consumption': 415.32206535339355, 'epoch': 0.54}

  0%|▎                                                                                                                                                           | 37/20000 [00:20<3:37:21,  1.53it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 942.9094791412354, 'epoch': 0.57}

  0%|▎                                                                                                                                                           | 39/20000 [00:22<4:06:59,  1.35it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 1096.3819026947021, 'epoch': 0.6}
{'peak_mem': 3.6841378211975098, 'step_consumption': 635.7886791229248, 'epoch': 0.62}
{'loss': 0.9475, 'learning_rate': 0.01, 'epoch': 0.63}

  0%|▎                                                                                                                                                           | 43/20000 [00:25<3:24:13,  1.63it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 815.7525062561035, 'epoch': 0.65}
{'peak_mem': 3.6841378211975098, 'step_consumption': 479.766845703125, 'epoch': 0.67}

  0%|▎                                                                                                                                                           | 46/20000 [00:27<3:34:16,  1.55it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 839.9264812469482, 'epoch': 0.7}
{'peak_mem': 3.6841378211975098, 'step_consumption': 831.5563201904297, 'epoch': 0.71}
{'peak_mem': 3.6841378211975098, 'step_consumption': 428.00068855285645, 'epoch': 0.73}

  0%|▍                                                                                                                                                           | 50/20000 [00:28<2:30:25,  2.21it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 372.93124198913574, 'epoch': 0.76}
{'peak_mem': 3.6841378211975098, 'step_consumption': 396.942138671875, 'epoch': 0.78}
{'loss': 0.9395, 'learning_rate': 0.01, 'epoch': 0.79}
{'peak_mem': 3.6841378211975098, 'step_consumption': 406.59046173095703, 'epoch': 0.79}

  0%|▍                                                                                                                                                           | 54/20000 [00:31<2:42:35,  2.04it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 734.8525524139404, 'epoch': 0.83}
{'peak_mem': 3.6841378211975098, 'step_consumption': 378.4809112548828, 'epoch': 0.84}
{'peak_mem': 3.6841378211975098, 'step_consumption': 351.38535499572754, 'epoch': 0.86}

  0%|▍                                                                                                                                                           | 61/20000 [00:33<1:50:57,  3.00it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 301.81884765625, 'epoch': 0.89}
{'peak_mem': 3.6841378211975098, 'step_consumption': 314.4378662109375, 'epoch': 0.9}
{'peak_mem': 3.6841378211975098, 'step_consumption': 335.9067440032959, 'epoch': 0.92}
{'peak_mem': 3.6841378211975098, 'step_consumption': 291.28241539001465, 'epoch': 0.94}
{'loss': 0.9328, 'learning_rate': 0.01, 'epoch': 0.95}
{'peak_mem': 3.6841378211975098, 'step_consumption': 304.5377731323242, 'epoch': 0.95}

  0%|▌                                                                                                                                                           | 67/20000 [00:35<1:39:42,  3.33it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 343.74046325683594, 'epoch': 0.98}
{'peak_mem': 3.6841378211975098, 'step_consumption': 273.09179306030273, 'epoch': 1.0}
-------------------------- Training Epoch 1 --------------------------
{'peak_mem': 3.6841378211975098, 'step_consumption': 304.16226387023926, 'epoch': 1.02}
{'peak_mem': 3.6841378211975098, 'step_consumption': 278.02491188049316, 'epoch': 1.03}
{'peak_mem': 3.6841378211975098, 'step_consumption': 299.3767261505127, 'epoch': 1.05}
{'peak_mem': 3.6841378211975098, 'step_consumption': 289.7460460662842, 'epoch': 1.06}
  0%|▌                                                                                                                                                           | 71/20000 [00:36<1:49:22,  3.04it/s][34m[1mwandb[39m[22m: 429 encountered (Filestream rate limit exceeded, retrying in 2.3 seconds.), retrying request
  0%|▌                                                                                                                                                           | 73/20000 [00:37<2:08:23,  2.59it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 345.2646732330322, 'epoch': 1.1}
{'loss': 0.9801, 'learning_rate': 0.01, 'epoch': 1.11}
{'peak_mem': 3.6841378211975098, 'step_consumption': 295.83191871643066, 'epoch': 1.11}
{'peak_mem': 3.6841378211975098, 'step_consumption': 350.23021697998047, 'epoch': 1.13}
{'peak_mem': 3.6841378211975098, 'step_consumption': 400.4812240600586, 'epoch': 1.14}

  0%|▌                                                                                                                                                           | 76/20000 [00:39<2:43:23,  2.03it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 527.6970863342285, 'epoch': 1.17}
{'peak_mem': 3.6841378211975098, 'step_consumption': 543.9622402191162, 'epoch': 1.19}

  0%|▌                                                                                                                                                           | 79/20000 [00:41<3:44:15,  1.48it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 842.2303199768066, 'epoch': 1.22}
{'peak_mem': 3.6841378211975098, 'step_consumption': 626.6422271728516, 'epoch': 1.24}

  0%|▋                                                                                                                                                           | 82/20000 [00:43<3:50:16,  1.44it/s]
{'loss': 0.9062, 'learning_rate': 0.01, 'epoch': 1.27}
{'peak_mem': 3.6841378211975098, 'step_consumption': 597.3100662231445, 'epoch': 1.27}
{'peak_mem': 3.6841378211975098, 'step_consumption': 924.6141910552979, 'epoch': 1.29}

  0%|▋                                                                                                                                                           | 84/20000 [00:44<3:44:45,  1.48it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 615.9617900848389, 'epoch': 1.32}

  0%|▋                                                                                                                                                           | 88/20000 [00:47<3:21:37,  1.65it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 969.6049690246582, 'epoch': 1.35}
{'peak_mem': 3.6841378211975098, 'step_consumption': 668.8807010650635, 'epoch': 1.37}
{'peak_mem': 3.6841378211975098, 'step_consumption': 615.0481700897217, 'epoch': 1.38}

  0%|▋                                                                                                                                                           | 93/20000 [00:49<2:29:47,  2.21it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 389.2378807067871, 'epoch': 1.41}
{'loss': 0.9333, 'learning_rate': 0.01, 'epoch': 1.43}
{'peak_mem': 3.6841378211975098, 'step_consumption': 390.02370834350586, 'epoch': 1.43}
{'peak_mem': 3.6841378211975098, 'step_consumption': 358.6714267730713, 'epoch': 1.44}
{'peak_mem': 3.6841378211975098, 'step_consumption': 452.52227783203125, 'epoch': 1.46}

  0%|▊                                                                                                                                                           | 99/20000 [00:51<1:58:13,  2.81it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 363.10887336730957, 'epoch': 1.49}
{'peak_mem': 3.6841378211975098, 'step_consumption': 303.38001251220703, 'epoch': 1.51}
{'peak_mem': 3.6841378211975098, 'step_consumption': 317.71326065063477, 'epoch': 1.52}
{'peak_mem': 3.6841378211975098, 'step_consumption': 349.2887020111084, 'epoch': 1.54}
{'peak_mem': 3.6841378211975098, 'step_consumption': 334.733247756958, 'epoch': 1.56}

  1%|▊                                                                                                                                                          | 105/20000 [00:53<1:50:13,  3.01it/s]
{'loss': 1.0739, 'learning_rate': 0.01, 'epoch': 1.59}
{'peak_mem': 3.6841378211975098, 'step_consumption': 334.92302894592285, 'epoch': 1.59}
{'peak_mem': 3.6841378211975098, 'step_consumption': 300.94170570373535, 'epoch': 1.6}
{'peak_mem': 3.6841378211975098, 'step_consumption': 295.1092720031738, 'epoch': 1.62}
{'peak_mem': 3.6841378211975098, 'step_consumption': 336.73834800720215, 'epoch': 1.63}
{'peak_mem': 3.6841378211975098, 'step_consumption': 297.0869541168213, 'epoch': 1.65}

  1%|▊                                                                                                                                                          | 111/20000 [00:55<1:54:12,  2.90it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 403.2750129699707, 'epoch': 1.68}
{'peak_mem': 3.6841378211975098, 'step_consumption': 345.17908096313477, 'epoch': 1.7}
{'peak_mem': 3.6841378211975098, 'step_consumption': 339.46943283081055, 'epoch': 1.71}
{'peak_mem': 3.6841378211975098, 'step_consumption': 344.5415496826172, 'epoch': 1.73}
{'loss': 0.9222, 'learning_rate': 0.01, 'epoch': 1.75}
{'peak_mem': 3.6841378211975098, 'step_consumption': 329.03122901916504, 'epoch': 1.75}

  1%|▉                                                                                                                                                          | 115/20000 [00:57<2:13:01,  2.49it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 386.17515563964844, 'epoch': 1.78}
{'peak_mem': 3.6841378211975098, 'step_consumption': 384.9008083343506, 'epoch': 1.79}
{'peak_mem': 3.6841378211975098, 'step_consumption': 438.0645751953125, 'epoch': 1.81}
{'peak_mem': 3.6841378211975098, 'step_consumption': 417.9248809814453, 'epoch': 1.83}

  1%|▉                                                                                                                                                          | 118/20000 [00:59<3:00:14,  1.84it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 629.0919780731201, 'epoch': 1.86}

  1%|▉                                                                                                                                                          | 121/20000 [01:01<3:33:58,  1.55it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 749.8340606689453, 'epoch': 1.89}
{'loss': 0.9715, 'learning_rate': 0.01, 'epoch': 1.9}
{'peak_mem': 3.6841378211975098, 'step_consumption': 728.9390563964844, 'epoch': 1.9}

  1%|▉                                                                                                                                                          | 124/20000 [01:03<3:40:38,  1.50it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 818.5908794403076, 'epoch': 1.94}
{'peak_mem': 3.6841378211975098, 'step_consumption': 697.1807479858398, 'epoch': 1.95}

  1%|▉                                                                                                                                                          | 126/20000 [01:05<4:38:48,  1.19it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 778.6808013916016, 'epoch': 1.98}
{'peak_mem': 3.6841378211975098, 'step_consumption': 1159.585952758789, 'epoch': 2.0}

  1%|▉                                                                                                                                                          | 129/20000 [01:07<4:02:06,  1.37it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 783.6639881134033, 'epoch': 2.02}
{'peak_mem': 3.6841378211975098, 'step_consumption': 822.8464126586914, 'epoch': 2.03}

  1%|█                                                                                                                                                          | 134/20000 [01:09<2:28:18,  2.23it/s]
{'loss': 0.9154, 'learning_rate': 0.01, 'epoch': 2.06}
{'peak_mem': 3.6841378211975098, 'step_consumption': 553.0436038970947, 'epoch': 2.06}
{'peak_mem': 3.6841378211975098, 'step_consumption': 377.3972988128662, 'epoch': 2.08}
{'peak_mem': 3.6841378211975098, 'step_consumption': 419.3761348724365, 'epoch': 2.1}
{'peak_mem': 3.6841378211975098, 'step_consumption': 378.24344635009766, 'epoch': 2.11}

  1%|█                                                                                                                                                          | 139/20000 [01:11<2:20:53,  2.35it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 361.5279197692871, 'epoch': 2.14}
{'peak_mem': 3.6841378211975098, 'step_consumption': 377.96664237976074, 'epoch': 2.16}
{'peak_mem': 3.6841378211975098, 'step_consumption': 369.37594413757324, 'epoch': 2.17}
{'peak_mem': 3.6841378211975098, 'step_consumption': 601.6879081726074, 'epoch': 2.19}

  1%|█                                                                                                                                                          | 144/20000 [01:13<2:07:46,  2.59it/s]
{'loss': 1.0475, 'learning_rate': 0.01, 'epoch': 2.22}
{'peak_mem': 3.6841378211975098, 'step_consumption': 402.1947383880615, 'epoch': 2.22}
{'peak_mem': 3.6841378211975098, 'step_consumption': 339.85042572021484, 'epoch': 2.24}
{'peak_mem': 3.6841378211975098, 'step_consumption': 409.31224822998047, 'epoch': 2.25}
{'peak_mem': 3.6841378211975098, 'step_consumption': 370.00465393066406, 'epoch': 2.27}

  1%|█▏                                                                                                                                                         | 150/20000 [01:15<1:53:45,  2.91it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 401.31378173828125, 'epoch': 2.3}
{'peak_mem': 3.6841378211975098, 'step_consumption': 299.8919486999512, 'epoch': 2.32}
{'peak_mem': 3.6841378211975098, 'step_consumption': 300.5962371826172, 'epoch': 2.33}
{'peak_mem': 3.6841378211975098, 'step_consumption': 346.91929817199707, 'epoch': 2.35}
{'peak_mem': 3.6841378211975098, 'step_consumption': 341.58849716186523, 'epoch': 2.37}
{'loss': 1.106, 'learning_rate': 0.01, 'epoch': 2.38}

  1%|█▏                                                                                                                                                         | 156/20000 [01:17<2:01:15,  2.73it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 361.3245487213135, 'epoch': 2.4}
{'peak_mem': 3.6841378211975098, 'step_consumption': 282.4437618255615, 'epoch': 2.41}
{'peak_mem': 3.6841378211975098, 'step_consumption': 345.379114151001, 'epoch': 2.43}
{'peak_mem': 3.6841378211975098, 'step_consumption': 351.87792778015137, 'epoch': 2.44}
{'peak_mem': 3.6841378211975098, 'step_consumption': 338.6378288269043, 'epoch': 2.46}

  1%|█▏                                                                                                                                                         | 161/20000 [01:19<2:09:18,  2.56it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 372.6534843444824, 'epoch': 2.49}
{'peak_mem': 3.6841378211975098, 'step_consumption': 364.55655097961426, 'epoch': 2.51}
{'peak_mem': 3.6841378211975098, 'step_consumption': 343.42050552368164, 'epoch': 2.52}
{'loss': 1.0143, 'learning_rate': 0.01, 'epoch': 2.54}
{'peak_mem': 3.6841378211975098, 'step_consumption': 360.4896068572998, 'epoch': 2.54}

  1%|█▎                                                                                                                                                         | 165/20000 [01:21<2:36:14,  2.12it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 350.6946563720703, 'epoch': 2.57}
{'peak_mem': 3.6841378211975098, 'step_consumption': 560.1763725280762, 'epoch': 2.59}
{'peak_mem': 3.6841378211975098, 'step_consumption': 538.2330417633057, 'epoch': 2.6}

  1%|█▎                                                                                                                                                         | 169/20000 [01:23<2:33:16,  2.16it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 487.398624420166, 'epoch': 2.63}
{'peak_mem': 3.6841378211975098, 'step_consumption': 447.8898048400879, 'epoch': 2.65}
{'peak_mem': 3.6841378211975098, 'step_consumption': 434.4785213470459, 'epoch': 2.67}

  1%|█▎                                                                                                                                                         | 173/20000 [01:25<2:51:52,  1.92it/s]
{'loss': 1.0667, 'learning_rate': 0.01, 'epoch': 2.7}
{'peak_mem': 3.6841378211975098, 'step_consumption': 558.250904083252, 'epoch': 2.7}
{'peak_mem': 3.6841378211975098, 'step_consumption': 543.4942245483398, 'epoch': 2.71}
{'peak_mem': 3.6841378211975098, 'step_consumption': 524.561882019043, 'epoch': 2.73}

  1%|█▍                                                                                                                                                         | 178/20000 [01:27<2:30:00,  2.20it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 454.3161392211914, 'epoch': 2.76}
{'peak_mem': 3.6841378211975098, 'step_consumption': 481.7984104156494, 'epoch': 2.78}
{'peak_mem': 3.6841378211975098, 'step_consumption': 403.66363525390625, 'epoch': 2.79}
{'peak_mem': 3.6841378211975098, 'step_consumption': 436.3586902618408, 'epoch': 2.81}

  1%|█▍                                                                                                                                                         | 182/20000 [01:29<2:26:30,  2.25it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 429.0273189544678, 'epoch': 2.84}
{'loss': 0.8073, 'learning_rate': 0.01, 'epoch': 2.86}
{'peak_mem': 3.6841378211975098, 'step_consumption': 449.8107433319092, 'epoch': 2.86}
{'peak_mem': 3.6841378211975098, 'step_consumption': 381.9737434387207, 'epoch': 2.87}

  1%|█▍                                                                                                                                                         | 187/20000 [01:31<2:16:06,  2.43it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 426.1136054992676, 'epoch': 2.9}
{'peak_mem': 3.6841378211975098, 'step_consumption': 459.8124027252197, 'epoch': 2.92}
{'peak_mem': 3.6841378211975098, 'step_consumption': 417.50121116638184, 'epoch': 2.94}
{'peak_mem': 3.6841378211975098, 'step_consumption': 430.8338165283203, 'epoch': 2.95}

  1%|█▍                                                                                                                                                         | 192/20000 [01:33<2:09:15,  2.55it/s]
{'peak_mem': 3.6841378211975098, 'step_consumption': 417.25850105285645, 'epoch': 2.98}
{'peak_mem': 3.6841378211975098, 'step_consumption': 451.6932964324951, 'epoch': 3.0}
-------------------------- Training Epoch 3 --------------------------
{'loss': 0.9098, 'learning_rate': 0.01, 'epoch': 3.02}
{'peak_mem': 3.6841378211975098, 'step_consumption': 365.6883239746094, 'epoch': 3.02}
{'peak_mem': 3.6841378211975098, 'step_consumption': 427.6912212371826, 'epoch': 3.03}
  1%|█▌                                                                                                                                                         | 194/20000 [01:34<2:00:26,  2.74it/s]Traceback (most recent call last):
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 736, in <module>
    main()
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 688, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 573, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 544, in _inner_training_loop
    tr_loss_step = self.zo_muon_step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 1097, in zo_muon_step
    loss1 = self.zo_forward(model, inputs)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 787, in zo_forward
    loss = self.compute_loss(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/ZO-LLM-main/zo-bench/utils.py", line 67, in forward_wrap_with_option_len
    outputs = self.original_forward(input_ids=input_ids, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 938, in forward
    outputs = self.model.decoder(
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 704, in forward
    layer_outputs = decoder_layer(
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 329, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 225, in forward
    attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))
KeyboardInterrupt
{'peak_mem': 3.6841378211975098, 'step_consumption': 317.5673484802246, 'epoch': 3.06}
{'peak_mem': 3.6841378211975098, 'step_consumption': 343.3196544647217, 'epoch': 3.08}