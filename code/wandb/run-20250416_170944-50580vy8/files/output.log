2025-04-16 17:09:53,863 - INFO - Sample train set 1500/67349
2025-04-16 17:09:53,863 - INFO - ... including dev set 500 samples
2025-04-16 17:09:53,864 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
78
2025-04-16 17:09:59,287 - INFO - Done with 5.42s
2025-04-16 17:09:59,583 - INFO - Dev samples: 500
2025-04-16 17:09:59,584 - INFO - Train samples: 1000
2025-04-16 17:09:59,584 - INFO - Eval sample length is 872
2025-04-16 17:09:59,584 - INFO - Tokenizing training samples...
2025-04-16 17:10:00,584 - INFO - Done with 1.00s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-04-16 17:10:00,597 - INFO - ***** Running training *****
2025-04-16 17:10:00,597 - INFO -   Num examples = 1000
2025-04-16 17:10:00,598 - INFO -   Num Epochs = 318
2025-04-16 17:10:00,598 - INFO -   Instantaneous batch size per device = 16
2025-04-16 17:10:00,598 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-04-16 17:10:00,598 - INFO -   Gradient Accumulation steps = 1
2025-04-16 17:10:00,598 - INFO -   Total optimization steps = 20000
2025-04-16 17:10:00,599 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                         | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
  0%|                                                                                                                                              | 1/20000 [00:08<47:49:39,  8.61s/it]

  0%|                                                                                                                                              | 2/20000 [00:18<50:22:58,  9.07s/it]

  0%|                                                                                                                                              | 3/20000 [00:27<51:13:28,  9.22s/it]
{'peak_mem': 5.889943599700928, 'step_consumption': 9399.150371551514, 'epoch': 0.05}


  0%|                                                                                                                                              | 5/20000 [00:46<51:53:24,  9.34s/it]

  0%|                                                                                                                                              | 6/20000 [00:55<52:08:00,  9.39s/it]

  0%|                                                                                                                                              | 7/20000 [01:04<51:32:27,  9.28s/it]

  0%|                                                                                                                                              | 8/20000 [01:13<51:15:03,  9.23s/it]

  0%|                                                                                                                                              | 9/20000 [01:22<50:53:43,  9.17s/it]

  0%|                                                                                                                                             | 10/20000 [01:31<50:40:19,  9.13s/it]
{'loss': 348.0897, 'learning_rate': 0.001, 'epoch': 0.16}
{'peak_mem': 5.889943599700928, 'step_consumption': 9046.89621925354, 'epoch': 0.16}


  0%|                                                                                                                                             | 12/20000 [01:50<50:41:26,  9.13s/it]
{'peak_mem': 5.890042781829834, 'step_consumption': 9179.445505142212, 'epoch': 0.19}


  0%|                                                                                                                                             | 14/20000 [02:08<51:11:25,  9.22s/it]

  0%|                                                                                                                                             | 15/20000 [02:17<51:07:03,  9.21s/it]

  0%|                                                                                                                                             | 16/20000 [02:27<50:50:18,  9.16s/it]

  0%|                                                                                                                                             | 17/20000 [02:36<50:52:41,  9.17s/it]

  0%|▏                                                                                                                                            | 18/20000 [02:45<51:01:17,  9.19s/it]

  0%|▏                                                                                                                                            | 19/20000 [02:54<50:51:54,  9.16s/it]

  0%|▏                                                                                                                                            | 20/20000 [03:03<50:42:56,  9.14s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.32}

  0%|▏                                                                                                                                            | 21/20000 [03:12<50:36:56,  9.12s/it]

  0%|▏                                                                                                                                            | 22/20000 [03:21<50:32:39,  9.11s/it]

  0%|▏                                                                                                                                            | 23/20000 [03:30<50:22:34,  9.08s/it]

  0%|▏                                                                                                                                            | 24/20000 [03:39<50:17:03,  9.06s/it]

  0%|▏                                                                                                                                            | 25/20000 [03:48<50:15:43,  9.06s/it]

  0%|▏                                                                                                                                            | 26/20000 [03:57<50:11:51,  9.05s/it]

  0%|▏                                                                                                                                            | 27/20000 [04:06<50:15:44,  9.06s/it]

  0%|▏                                                                                                                                            | 28/20000 [04:15<50:11:26,  9.05s/it]

  0%|▏                                                                                                                                            | 29/20000 [04:25<50:15:04,  9.06s/it]

  0%|▏                                                                                                                                            | 30/20000 [04:34<50:10:25,  9.04s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.48}

  0%|▏                                                                                                                                            | 31/20000 [04:43<50:11:03,  9.05s/it]

  0%|▏                                                                                                                                            | 32/20000 [04:52<50:11:57,  9.05s/it]

  0%|▏                                                                                                                                            | 33/20000 [05:01<50:19:09,  9.07s/it]

  0%|▏                                                                                                                                            | 34/20000 [05:10<50:14:49,  9.06s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9027.816772460938, 'epoch': 0.54}


  0%|▎                                                                                                                                            | 36/20000 [05:28<50:17:47,  9.07s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9055.6321144104, 'epoch': 0.57}


  0%|▎                                                                                                                                            | 38/20000 [05:46<50:14:42,  9.06s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9056.968927383423, 'epoch': 0.6}


  0%|▎                                                                                                                                            | 40/20000 [06:04<50:13:11,  9.06s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.63}
{'peak_mem': 6.127905368804932, 'step_consumption': 9073.741436004639, 'epoch': 0.63}


  0%|▎                                                                                                                                            | 42/20000 [06:22<50:11:36,  9.05s/it]

  0%|▎                                                                                                                                            | 43/20000 [06:31<50:09:25,  9.05s/it]

  0%|▎                                                                                                                                            | 44/20000 [06:40<50:07:18,  9.04s/it]

  0%|▎                                                                                                                                            | 45/20000 [06:49<50:11:06,  9.05s/it]

  0%|▎                                                                                                                                            | 46/20000 [06:58<50:13:16,  9.06s/it]

  0%|▎                                                                                                                                            | 47/20000 [07:08<50:11:43,  9.06s/it]

  0%|▎                                                                                                                                            | 48/20000 [07:17<50:14:45,  9.07s/it]

  0%|▎                                                                                                                                            | 49/20000 [07:26<50:15:48,  9.07s/it]

  0%|▎                                                                                                                                            | 50/20000 [07:35<50:15:15,  9.07s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.79}

  0%|▎                                                                                                                                            | 51/20000 [07:44<50:14:49,  9.07s/it]

  0%|▎                                                                                                                                            | 52/20000 [07:53<50:13:18,  9.06s/it]

  0%|▎                                                                                                                                            | 53/20000 [08:02<50:13:10,  9.06s/it]

  0%|▍                                                                                                                                            | 54/20000 [08:11<50:13:46,  9.07s/it]

  0%|▍                                                                                                                                            | 55/20000 [08:20<50:18:25,  9.08s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9111.446857452393, 'epoch': 0.87}


  0%|▍                                                                                                                                            | 57/20000 [08:38<50:10:06,  9.06s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9029.970645904541, 'epoch': 0.9}


  0%|▍                                                                                                                                            | 59/20000 [08:56<50:08:50,  9.05s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9045.67003250122, 'epoch': 0.94}
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 0.95}


  0%|▍                                                                                                                                            | 61/20000 [09:14<50:07:25,  9.05s/it]

  0%|▍                                                                                                                                            | 62/20000 [09:23<50:09:59,  9.06s/it]

  0%|▍                                                                                                                                            | 63/20000 [09:32<50:05:53,  9.05s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9014.256477355957, 'epoch': 1.0}

  0%|▍                                                                                                                                            | 64/20000 [09:42<50:04:57,  9.04s/it]

  0%|▍                                                                                                                                            | 65/20000 [09:51<50:02:46,  9.04s/it]

  0%|▍                                                                                                                                            | 66/20000 [10:00<50:02:30,  9.04s/it]

  0%|▍                                                                                                                                            | 67/20000 [10:09<50:02:54,  9.04s/it]

  0%|▍                                                                                                                                            | 68/20000 [10:18<50:06:50,  9.05s/it]

  0%|▍                                                                                                                                            | 69/20000 [10:27<50:09:02,  9.06s/it]

  0%|▍                                                                                                                                            | 70/20000 [10:36<50:04:17,  9.04s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 1.11}

  0%|▌                                                                                                                                            | 71/20000 [10:45<50:02:38,  9.04s/it]

  0%|▌                                                                                                                                            | 72/20000 [10:54<50:01:13,  9.04s/it]

  0%|▌                                                                                                                                            | 73/20000 [11:03<50:06:10,  9.05s/it]

  0%|▌                                                                                                                                            | 74/20000 [11:12<50:11:09,  9.07s/it]

  0%|▌                                                                                                                                            | 75/20000 [11:21<50:13:03,  9.07s/it]

  0%|▌                                                                                                                                            | 76/20000 [11:30<50:12:26,  9.07s/it]

  0%|▌                                                                                                                                            | 77/20000 [11:39<50:08:11,  9.06s/it]

  0%|▌                                                                                                                                            | 78/20000 [11:48<50:09:19,  9.06s/it]

  0%|▌                                                                                                                                            | 79/20000 [11:57<50:08:38,  9.06s/it]

  0%|▌                                                                                                                                            | 80/20000 [12:06<50:04:51,  9.05s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 1.27}

  0%|▌                                                                                                                                            | 81/20000 [12:15<50:08:31,  9.06s/it]

  0%|▌                                                                                                                                            | 82/20000 [12:25<50:06:43,  9.06s/it]

  0%|▌                                                                                                                                            | 83/20000 [12:34<50:05:51,  9.06s/it]

  0%|▌                                                                                                                                            | 84/20000 [12:43<50:04:59,  9.05s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9044.909238815308, 'epoch': 1.33}


  0%|▌                                                                                                                                            | 86/20000 [13:01<50:04:46,  9.05s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9058.939456939697, 'epoch': 1.37}


  0%|▌                                                                                                                                            | 88/20000 [13:19<50:10:13,  9.07s/it]

  0%|▋                                                                                                                                            | 89/20000 [13:28<50:09:24,  9.07s/it]

  0%|▋                                                                                                                                            | 90/20000 [13:37<50:09:32,  9.07s/it]
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 1.43}

  0%|▋                                                                                                                                            | 91/20000 [13:46<50:29:24,  9.13s/it]

  0%|▋                                                                                                                                            | 92/20000 [13:55<50:23:38,  9.11s/it]

  0%|▋                                                                                                                                            | 93/20000 [14:04<50:17:41,  9.10s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9051.475524902344, 'epoch': 1.48}


  0%|▋                                                                                                                                            | 95/20000 [14:23<50:06:45,  9.06s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9005.788803100586, 'epoch': 1.51}


  0%|▋                                                                                                                                            | 97/20000 [14:41<50:02:55,  9.05s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9059.615850448608, 'epoch': 1.54}


  0%|▋                                                                                                                                            | 99/20000 [14:59<50:01:56,  9.05s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9049.98230934143, 'epoch': 1.57}
{'loss': 0.0, 'learning_rate': 0.001, 'epoch': 1.59}


  1%|▋                                                                                                                                           | 101/20000 [15:17<49:55:09,  9.03s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9012.312412261963, 'epoch': 1.6}


  1%|▋                                                                                                                                           | 103/20000 [15:35<49:55:33,  9.03s/it]
{'peak_mem': 6.127905368804932, 'step_consumption': 9047.016143798828, 'epoch': 1.63}


  1%|▋                                                                                                                                           | 105/20000 [15:53<49:56:32,  9.04s/it]
  1%|▋                                                                                                                                           | 105/20000 [15:53<49:56:32,  9.04s/it]Traceback (most recent call last):
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 736, in <module>
    main()
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 688, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/ZO-LLM-main/zo-bench/run.py", line 573, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 544, in _inner_training_loop
    tr_loss_step = self.zo_muon_step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 1134, in zo_muon_step
    g_ortho = zeropower_via_newtonschulz5(g, steps=10)
  File "/home/rinya/ZO-LLM-main/zo-bench/trainer.py", line 147, in zeropower_via_newtonschulz5
    X = a * X + B @ X
KeyboardInterrupt